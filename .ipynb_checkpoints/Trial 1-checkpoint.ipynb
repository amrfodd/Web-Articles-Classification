{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d32f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f335db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"sci.med\", \"sci.space\"]\n",
    "\n",
    "X_train, y_train = fetch_20newsgroups(\n",
    "    random_state=1,\n",
    "    subset=\"train\",\n",
    "    categories=categories,\n",
    "    remove=(\"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")\n",
    "X_test, y_test = fetch_20newsgroups(\n",
    "    random_state=1,\n",
    "    subset=\"test\",\n",
    "    categories=categories,\n",
    "    remove=(\"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b4c0ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27b7ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\"Text\": X_train, \"Sentiment\": y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32e85aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: mccall@mksol.dseg.ti.com (fred j mccall ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mary@uicsl.csl.uiuc.edu (Mary E. Allison...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: henry@zoo.toronto.edu (Henry Spencer)\\nS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: geb@cs.pitt.edu (Gordon Banks)\\nSubject:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: dgempey@ucscb.UCSC.EDU (David Gordon Emp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  From: mccall@mksol.dseg.ti.com (fred j mccall ...          1\n",
       "1  From: mary@uicsl.csl.uiuc.edu (Mary E. Allison...          0\n",
       "2  From: henry@zoo.toronto.edu (Henry Spencer)\\nS...          1\n",
       "3  From: geb@cs.pitt.edu (Gordon Banks)\\nSubject:...          0\n",
       "4  From: dgempey@ucscb.UCSC.EDU (David Gordon Emp...          1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52ae99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293c8fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(train.target_names)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d05966b",
   "metadata": {},
   "source": [
    "Next we Vectorize the articles in the Corpus. For this we use sci-kit learn's CountVectorizer to create a sparse matrix of the count of each word in an article For better results we then calculate the inverse term frequency for the words using sci-kit learn's TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f976340f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to vectorize the training data: 5.386 secs\n",
      "Sample sparse matrix after vectorization:\n",
      "  (0, 18299)\t0.138749083899155\n",
      "  (0, 16574)\t0.14155752531572685\n",
      "  (0, 4605)\t0.06332603952480323\n",
      "  (1, 7797)\t0.13724375024886207\n",
      "  (1, 2927)\t0.05212944077716301\n",
      "  (2, 15032)\t0.07834044496813064\n",
      "  (2, 12197)\t0.05168179280403426\n",
      "  (2, 6449)\t0.06812813848609162\n",
      "  (2, 6028)\t0.10554465088856507\n",
      "  (2, 5811)\t0.2878251559842457\n",
      "  (2, 5023)\t0.13698619641739626\n",
      "  (2, 3412)\t0.06228731252083091\n",
      "  (3, 18618)\t0.14195950717692904\n",
      "  (3, 4155)\t0.05353413616615428\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "t1=time.time()\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print (\"Time take to vectorize the training data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Sample sparse matrix after vectorization:\")\n",
    "print (X_train_tfidf[0:4,0:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91549199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to Vectorize training data and train model: 4.209 secs\n",
      "Time take to Predict classes for testing data: 12.999 secs\n",
      "Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6591874668082847"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import neighbors\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', neighbors.KNeighborsClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy:\")\n",
    "np.mean(predicted == test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652359a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to Vectorize training data and train model: 5.899 secs\n",
      "Time take to Predict classes for testing data: 2.455 secs\n",
      "Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8534253850238981"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy:\")\n",
    "np.mean(predicted == test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6464eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to Vectorize training data and train model: 4.271 secs\n",
      "Time take to Predict classes for testing data: 2.301 secs\n",
      "Accuracy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy:\")\n",
    "np.mean(predicted == test.target)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d8efb3b",
   "metadata": {},
   "source": [
    "All this while we used Bag-Of-Words technique to vectorize the dataset. Here we apply ngrams technique to create the sparse matrix. Let's have an example as how n-grams is differnt from Bag-of-Words and what it actually does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d28047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words\n",
      "['anagh', 'anmol', 'intelligent', 'is', 'smart']\n",
      "[[2 0 1 1 0]\n",
      " [0 1 0 1 1]]\n",
      "Bi-grams\n",
      "[' a', 'ag', 'an', 'gh', 'h ', 'l ', 'mo', 'na', 'nm', 'ol']\n",
      "[[1 1 1 1 1 0 0 1 0 0]\n",
      " [1 0 1 0 0 1 1 0 1 1]]\n",
      "Tri-grams\n",
      "[' an', 'agh', 'ana', 'anm', 'gh ', 'mol', 'nag', 'nmo', 'ol ']\n",
      "[[1 1 1 0 1 0 1 0 0]\n",
      " [1 0 0 1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer()\n",
    "counts = ngram_vectorizer.fit_transform(['Anagh Anagh is intelligent', 'Anmol is smart'])\n",
    "print(\"Bag-of-Words\")\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "print(counts.toarray().astype(int))\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n",
    "counts = ngram_vectorizer.fit_transform(['Anagh', 'Anmol'])\n",
    "print(\"Bi-grams\")\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "print(counts.toarray().astype(int))\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))\n",
    "counts = ngram_vectorizer.fit_transform(['Anagh', 'Anmol'])\n",
    "print(\"Tri-grams\")\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "print(counts.toarray().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64721028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-2grams\n",
      "['be prayagraj', 'is allahabad', 'it is', 'it will', 'today it', 'tomorrow it', 'will be']\n",
      "[[0 1 1 0 1 0 0]\n",
      " [1 0 0 1 0 1 1]]\n",
      "Time take to vectorize the training data: 20.255 secs\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "counts = ngram_vectorizer.fit_transform(['Today it is Allahabad', 'Tomorrow it will be Prayagraj'])\n",
    "print(\"Bag-of-2grams\")\n",
    "print(ngram_vectorizer.get_feature_names())\n",
    "print(counts.toarray().astype(int))\n",
    "t1=time.time()\n",
    "count_vect = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print (\"Time take to vectorize the training data:\", round(time.time()-t1, 3), \"secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2beb3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR CHARACTER LEVEL n-GRAMS\n",
      "-------------------\n",
      "UNI-GRAMS\n",
      "Time take to Vectorize training data and train model: 16.198 secs\n",
      "Accuracy :  0.21959638874137016\n",
      "--------------------\n",
      "BI-GRAMS\n",
      "Time take to Vectorize training data and train model: 19.729 secs\n",
      "Accuracy :  0.6658258098778544\n",
      "--------------------\n",
      "TRI-GRAMS\n",
      "Time take to Vectorize training data and train model: 24.296 secs\n",
      "Accuracy :  0.816250663834307\n",
      "--------------------\n",
      "4-GRAMS\n",
      "Time take to Vectorize training data and train model: 25.887 secs\n",
      "Accuracy :  0.8430695698353691\n",
      "--------------------\n",
      "5-GRAMS\n",
      "Time take to Vectorize training data and train model: 26.204 secs\n",
      "Time take to Vectorize training data and train model: 9.687 secs\n",
      "Accuracy :  0.8377588953797133\n",
      "--------------------\n",
      "6-GRAMS\n",
      "Time take to Vectorize training data and train model: 25.408 secs\n",
      "Accuracy :  0.8347052575677111\n"
     ]
    }
   ],
   "source": [
    "print(\"FOR CHARACTER LEVEL n-GRAMS\")\n",
    "print(\"-------------------\")\n",
    "print(\"UNI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(1, 1))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"BI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "\n",
    "print(\"--------------------\")\n",
    "print(\"TRI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"4-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(4, 4))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"5-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"6-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(6, 6))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f82b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we apply N-Grams with Naive Bayes Classifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import neighbors\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to Predict classes for Test data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35728b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we apply N-Grams with KNN classifier\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', neighbors.KNeighborsClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "t1=time.time()\n",
    "\n",
    "predicted = text_clf.predict(test.data)\n",
    "print (\"Time take to predict classes for Test data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f847d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now applying Bag-of-1gram(Same as bag of words), Bag-of-2grams and Bag-of-3grams to our dataset this time grouping words together instead of characters\n",
    "\n",
    "print(\"WORD LEVEL n-GRAMS\")\n",
    "print(\"--------------------\")\n",
    "print(\"UNI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(1, 1))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"BI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(2, 2))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))\n",
    "print(\"--------------------\")\n",
    "print(\"TRI-GRAMS\")\n",
    "text_clf = Pipeline([('vect', CountVectorizer(analyzer='word', ngram_range=(3, 3))),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "t0=time.time()\n",
    "text_clf.fit(train.data, train.target)\n",
    "print (\"Time take to Vectorize training data and train model:\", round(time.time()-t0, 3), \"secs\")\n",
    "predicted = text_clf.predict(test.data)\n",
    "print(\"Accuracy : \",np.mean(predicted == test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05278e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we apply Word level CNN on the same dataset and calculate accuracy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "np.random.seed(1237)\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "label_index = train.target\n",
    "label_names = train.target_names\n",
    "labelled_files = train.filenames\n",
    " \n",
    "data_tags = [\"filename\",\"category\",\"news\"]\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],train.data[i]))\n",
    "    i += 1\n",
    "    \n",
    "train_data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
    "\n",
    "label_index = test.target\n",
    "label_names = test.target_names\n",
    "labelled_files = test.filenames\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],test.data[i]))\n",
    "    i += 1\n",
    "    \n",
    "test_data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
    "\n",
    "train_posts = train_data['news'][:]\n",
    "train_tags = train_data['category'][:]\n",
    "train_files_names = train_data['filename'][:]\n",
    " \n",
    "test_posts = test_data['news'][:]\n",
    "test_tags = test_data['category'][:]\n",
    "test_files_names = test_data['filename'][:]\n",
    "\n",
    "# 20 news groups\n",
    "num_labels = 20\n",
    "vocab_size = 50000\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    " \n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    " \n",
    "\n",
    "\t\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.333))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "t1=time.time()\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we apply CNN on data pre processed with N-Grams, taking value of N as 2.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "np.random.seed(1237)\n",
    "import time\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer='word', ngram_range=(2, 2), max_features=30000)\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', ngram_range=(2, 2), max_features=30000)\n",
    "X_train_counts = count_vect.fit_transform(test.data)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "label_index = train.target\n",
    "label_names = train.target_names\n",
    "labelled_files = train.filenames\n",
    " \n",
    "data_tags = [\"filename\",\"category\",\"news\"]\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],train.data[i]))\n",
    "    i += 1\n",
    "    \n",
    "train_data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
    "\n",
    "label_index = test.target\n",
    "label_names = test.target_names\n",
    "labelled_files = test.filenames\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],test.data[i]))\n",
    "    i += 1\n",
    "    \n",
    "test_data = pd.DataFrame.from_records(data_list, columns=data_tags)\n",
    "\n",
    "train_posts = train_data['news'][:]\n",
    "train_tags = train_data['category'][:]\n",
    "train_files_names = train_data['filename'][:]\n",
    " \n",
    "test_posts = test_data['news'][:]\n",
    "test_tags = test_data['category'][:]\n",
    "test_files_names = test_data['filename'][:]\n",
    "\n",
    "# 20 news groups\n",
    "num_labels = 20\n",
    "vocab_size = 50000\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    " \n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    " \n",
    "\n",
    "\t\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(30000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.333))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(X_train_tfidf.todense(), y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=20,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "t1=time.time()\n",
    "score = model.evaluate(X_test_tfidf, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print (\"Time take to Predict classes for testing data:\", round(time.time()-t1, 3), \"secs\")\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2646729d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980f376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d50be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afc53d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374af261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e2850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24302a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca1a729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3042b8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9f9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a08b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
