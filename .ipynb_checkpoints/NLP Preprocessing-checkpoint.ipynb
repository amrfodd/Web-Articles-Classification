{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad9b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "Having too many typos or spelling mistakes in the text\n",
    "Having too many numbers and punctuations (E.g. Love!!!!)\n",
    "Text is full of emojis and emoticons and username and links too. (If the text is from Twitter or Facebook)\n",
    "Some of the text parts are not in the English language. Data is having a mixture of more than one language\n",
    "Some of the words are combined with the hyphen or data having contractions words. (E.g. text-processing)\n",
    "Repetitions of words (E.g. Data)\n",
    "Well, honestly there are many more things that a trained eye can see. But if we look in general and just want an overview then follow the article for it.\n",
    "\n",
    "Most common methods for Cleaning the Data\n",
    "\n",
    "Lowecasing the data\n",
    "Removing Puncuatations\n",
    "Removing Numbers\n",
    "Removing extra space\n",
    "Replacing the repetitions of punctations\n",
    "Removing Emojis\n",
    "Removing emoticons\n",
    "Removing Contractions\n",
    "Removing HTML tags\n",
    "Removing & Finding URL\n",
    "Removing & Finding Email id\n",
    "Removing Stop Words\n",
    "Standardizing and Spell Check\n",
    "Chat word correction\n",
    "Remove the frequent words\n",
    "Removing the less frequent words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35a96d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amr_a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a258f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '‚ÄúHey Amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first PLEASE FIX ASAP! @AmazonHelp‚Äù'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7ded48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‚Äúhey amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first please fix asap! @amazonhelp‚Äù'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Normalizing Text \n",
    "\n",
    "def normalize(txt):\n",
    "    txt = txt.lower()\n",
    "    return txt\n",
    "\n",
    "normalize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452babae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Remove URLs and E-mails (UniCode)\n",
    "\n",
    "def remove_unicode(content):\n",
    "    content = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Removing Unicode Characters\n",
    "\n",
    "def unicode(txt):\n",
    "    txt = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", txt)\n",
    "    return txt\n",
    "\n",
    "unicode(text)\n",
    "\n",
    "# 3. Removing Stopwords\n",
    "\n",
    "def stopwords(txt):\n",
    "    txt = \" \".join([word for word in txt.split() if word not in (stop)])\n",
    "    return txt\n",
    "\n",
    "unicode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ae5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def  clean_text(df, text_field, new_text_field_name):\n",
    "    df[new_text_field_name] = df[text_field].str.lower()\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    # remove numbers\n",
    "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff113bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "data_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return wordstrain['headline_text'] = train['headline_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ada5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Remove URLs\n",
    "def remove_URL(headline_text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', headline_text)\n",
    "\n",
    "df.a = df['a'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a75b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Removing HTML tags\n",
    "def remove_html(headline_text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',headline_text)\n",
    "\n",
    "train['headline_text'] = train['headline_text'].apply(remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Removing Pictures/Tags/Symbols/Emojis\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "train['headline_text'] = train['headline_text'].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"U0001F600-U0001F64F\"  # emoticons\n",
    "                           u\"U0001F300-U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"U0001F680-U0001F6FF\"  # transport & map symbols\n",
    "                           u\"U0001F1E0-U0001F1FF\"  # flags (iOS)\n",
    "                           u\"U00002702-U000027B0\"\n",
    "                           u\"U000024C2-U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "remove_emoji(\"game is on üî•üî•\")\n",
    "#Output\n",
    "'game is on '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3b6758b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '‚Äù' (U+201D) (Temp/ipykernel_3344/2049390569.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\amr_a\\AppData\\Local\\Temp/ipykernel_3344/2049390569.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    u‚Äù:‚Äë)‚Äù:‚ÄùHappy face or smiley‚Äù,\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '‚Äù' (U+201D)\n"
     ]
    }
   ],
   "source": [
    "# Removing of Emoticons\n",
    "EMOTICONS = {\n",
    "u‚Äù:‚Äë)‚Äù:‚ÄùHappy face or smiley‚Äù,\n",
    "u‚Äù:)‚Äù:‚ÄùHappy face or smiley‚Äù,\n",
    "u‚Äù:-]‚Äù:‚ÄùHappy face or smiley‚Äù,\n",
    "u‚Äù:]‚Äù:‚ÄùHappy face or smiley‚Äù,\n",
    "u‚Äù:-3‚Ä≥:‚ÄùHappy face smiley‚Äù,\n",
    "u‚Äù:3‚Ä≥:‚ÄùHappy face smiley‚Äù,\n",
    "u‚Äù:->‚Äù:‚ÄùHappy face smiley‚Äù,\n",
    "u‚Äù:>‚Äù:‚ÄùHappy face smiley‚Äù,\n",
    "u‚Äù8-)‚Äù:‚ÄùHappy face smiley‚Äù,\n",
    "u‚Äù:o)‚Äù:‚ÄùHappy face smiley‚Äù,\n",
    "u‚Äù:-}‚Äù:‚ÄùHappy face smiley‚Äù,\n",
    "u‚Äù:}‚Äù:‚ÄùHappy face smiley‚Äù,\n",
    "u‚Äù:-)‚Äù:‚ÄùHappy face smiley‚Äù,\n",
    "u‚Äù:c)‚Äù:‚ÄùHappy face smiley‚Äù,\n",
    "u‚Äù:^)‚Äù:‚ÄùHappy face smiley‚Äù,\n",
    "u‚Äù=]‚Äù:‚ÄùHappy face smiley‚Äù\n",
    "}\n",
    "text = ‚ÄòI had such high hopes for this dress 15 size really wanted it to work for me :-)‚Äô\n",
    "ans = re.compile(u'(‚Äò + u‚Äô|‚Äô.join(k for k in EMOTICONS) + u‚Äô)‚Äô)\n",
    "ans = ans.sub(r‚Äù,text)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895051d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "text = \"Hello :-)\"\n",
    "convert_emoticons(text)\n",
    "#Output\n",
    "'Hello Happy_face_smiley'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuation\n",
    "def remove_punct(headline_text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return headline_text.translate(table)\n",
    "train['headline_text'] = train['headline_text'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "text = ‚ÄúI had such high hopes! for this dress size or (my usual size) to work for me.‚Äù\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "ans = text.translate(str.maketrans(‚Äù, ‚Äù, PUNCT_TO_REMOVE))\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82284a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c6928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = ''.join([i for i in text if not i.isdigit()])\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf956bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  4) cleaning digits\n",
    "def clean_numbers(content):\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    text = content.translate(remove_digits)\n",
    "    return text\n",
    "\n",
    "clean_numbers(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra space\n",
    "\n",
    "ans = \" \".join(text.split())\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85905dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the Repetitions of Punctuations\n",
    "text1 = \"I had such... high hopes for this dress!!!!\"\n",
    "ans = re.sub(r'(!)1+', '', text1)\n",
    "ans\n",
    "#Output\n",
    "'I had such... high hopes for this dress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc9c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text1 = \"I had such... high hopes for this dress!!!!\" \n",
    "ans = re.sub(r'(!|.)1+', '', text1) \n",
    "ans\n",
    "#Output\n",
    "'I had such high hopes for this dress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58847b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import contractions\n",
    "text1 = \"She'd like to know how I'd do that!\"\n",
    "contractions.fix(text1)\n",
    "#Output\n",
    "she would like to know how I would do that!Removing Contractions\n",
    "\n",
    "tex2t = \"She'd like to know how I'd do that!\"\n",
    "contractions.fix(text2)\n",
    "#Output\n",
    "she would like to know how I would do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e12f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML\n",
    "‚ÄúI had such high hopes for this dress 15 size or (my usual size) to work for me.‚Äù\n",
    "without_html = re.sub(pattern=r‚Äù, repl=‚Äô ‚Äò, string=text)\n",
    "print(f‚Äù{without_html}‚Äù)\n",
    "\n",
    "# Removal of HTML tags\n",
    "def clean_html(content):\n",
    "    # content: str\n",
    "    # return: str\n",
    "    reg_html = '<.*?>'\n",
    "    html_pattern = re.compile(reg_html)\n",
    "    content = html_pattern.sub(r'', text)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5f56e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdfsdafsaf code style code a aefdsafsf'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(html):\n",
    "\n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for data in soup(['style', 'script', 'code', 'a']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "\n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "\n",
    "html = \"\"\"<p> sdfsdafsaf<li>code style code a aefdsafsf</li>\"\"\"\n",
    "clean_html(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29333997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amr_a\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I high hopes dress 1-5 size work me.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopword\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "text = \"I had such high hopes for this dress 1-5 size to work for me.\" \n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "ans = \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ac76dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Text: A farmmer will lovdd this food\n",
      "After correcting text: A farmer will loved this food\n"
     ]
    }
   ],
   "source": [
    "## Standardizing and Spell Check\n",
    "\n",
    "import itertools\n",
    "from autocorrect import Speller\n",
    "text=\"A farmmer will lovdd this food\"\n",
    "#One letter in a word should not be present more than twice in continuation\n",
    "\n",
    "text_correction = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "print(\"Normal Text: {}\".format(text_correction))\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "ans = spell(text_correction)\n",
    "\n",
    "print(\"After correcting text: {}\".format(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a81323bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3344/3344526005.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mcw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"=\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mcw_expanded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"=\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mchat_words_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mchat_words_map_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcw_expanded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Chat Words Conversion\n",
    "chat_words_str = \"\"\" \n",
    "AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\"\"\"\n",
    "chat_words_map_dict = {}\n",
    "chat_words_list = []\n",
    "for line in chat_words_str.split(\"n\"):\n",
    "    if line != \"\":\n",
    "        cw = line.split(\"=\")[0]\n",
    "        cw_expanded = line.split(\"=\")[1]\n",
    "        chat_words_list.append(cw)\n",
    "        chat_words_map_dict[cw] = cw_expanded\n",
    "chat_words_list = set(chat_words_list)\n",
    "def chat_words_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words_list:\n",
    "            new_text.append(chat_words_map_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "chat_words_conversion(\"one minute A3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95bc46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the Frequent Words\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df[\"text\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "cnt.most_common(10)\n",
    "\n",
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "def remove_freqwords(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "df[\"text_wo_stopfreq\"] = df[\"text\"].apply(lambda text: remove_freqwords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc1e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the Less Frequent Words\n",
    "\n",
    "n_rare_words = 10\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "def remove_rarewords(text):\n",
    "    \"\"\"custom function to remove the rare words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "df[\"text_wo_stopfreqrare\"] = df[\"text_wo_stopfreq\"].apply(lambda text: remove_rarewords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0c122d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ab7be0b4",
   "metadata": {},
   "source": [
    "## \n",
    "Make the text lowercase. As you probably know, NLP is case-sensitive.\n",
    "Remove line breaks. Again, depending on your source, you might have encoded line breaks.\n",
    "Remove punctuation. This is using the string library. Other punctuation can be added as needed.\n",
    "Remove stop words using the NLTK library. There is a list in the next line to add additional stop words to the function as needed. These might be noisy domain words or anything else that makes the contextless clear.\n",
    "Removing numbers. Optional depending on your data.\n",
    "Stemming or Lemmatization. This process is an argument in the function. You can choose either one via with Stem or Lem. The default is to use none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7797b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_string(text, stem=\"None\"):\n",
    "\n",
    "    final_string = \"\"\n",
    "\n",
    "    # Make lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove line breaks\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "\n",
    "    # Remove puncuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    useless_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    useless_words = useless_words + ['hi', 'im']\n",
    "\n",
    "    text_filtered = [word for word in text if not word in useless_words]\n",
    "\n",
    "    # Remove numbers\n",
    "    text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
    "\n",
    "    # Stem or Lemmatize\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer() \n",
    "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
    "    elif stem == 'Lem':\n",
    "        lem = WordNetLemmatizer()\n",
    "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
    "    elif stem == 'Spacy':\n",
    "        text_filtered = nlp(' '.join(text_filtered))\n",
    "        text_stemmed = [y.lemma_ for y in text_filtered]\n",
    "    else:\n",
    "        text_stemmed = text_filtered\n",
    "\n",
    "    final_string = ' '.join(text_stemmed)\n",
    "\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a474173",
   "metadata": {},
   "outputs": [],
   "source": [
    "<p><a href=\"\"\"https://forge.autodesk.com/en/docs/data/v2/tutorials/download-file/#step-6-download-the-item\" rel=\"nofollow noreferrer\">https://forge.autodesk.com/en/docs/data/v2/tutorials/download-file/#step-6-download-the-item</a></p>\\n\\n<p>I have followed the tutorial and have successfully obtained the contents of the file, but where is the file being downloaded. In addition, how do I specify the location of where I want to download the file?</p>\\n\\n<p>Result on Postman\\n<a href=\"https://i.stack.imgur.com/VrdqP.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/VrdqP.png\" alt=\"enter image description here\"></a></p>\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d0b4728",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3344/4095299909.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# To remove HTML first and apply it directly to the source text column.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclean_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# To remove HTML first and apply it directly to the source text column.\n",
    "df['body'] = df['body'].apply(lambda x: clean_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a357a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next apply the clean_string function to the text\n",
    "df['body_clean'] = df['body'].apply(lambda x: clean_string(x, stem='Stem'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preproces(x):\n",
    "    \"\"\"\n",
    "    Function: Replace signs to strings and remove\n",
    "    Args:\n",
    "      x: pandas series\n",
    "    Return:\n",
    "      x: pandas series\n",
    "    \"\"\"\n",
    "    \n",
    "    x = str(x).lower()\n",
    "    x = x.replace('%',' percent').replace('‚Çπ',' rupee').replace('$',' dollar').replace('‚Ç¨',' euro')\\\n",
    "                                .replace(',000,000','m').replace('000','k').replace('‚Ä≤',\"'\").replace(\"‚Äô\",\"'\")\\\n",
    "                                .replace(\"won't\",\"will not\").replace(\"can't\",'can not').replace(\"shouldn't\",\"should not\")\\\n",
    "                                .replace(\"what's\",'\"what is\"').replace(\"that's\",'that is').replace(\"he's\",\"he is\")\\\n",
    "                                .replace(\"she's\",\"she is\").replace(\"it's\",\"it is\").replace(\"'ve\",\" have\").replace(\"'re\",\" are\")\\\n",
    "                                .replace(\"'ll\",\" will\").replace(\"i'm\",\"i am\").replace(\"n't\", \" not\")\n",
    "    x = re.sub(r'([0-9]+)000000',r'\\1m',x)\n",
    "    x = re.sub(r'([0-9]+)000',r'\\1k',x)\n",
    "    \n",
    "    return x    \n",
    "\n",
    "def extract_features(df):\n",
    "    df['Summary'] = df['Summary'].fillna(\"\").apply(text_preproces)\n",
    "    print(\"token features...\")\n",
    "    \n",
    "    return df\n",
    "df = extract_features(df)\n",
    "token features...\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a8126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.1. Text Exploration\n",
    "# Find the common words in each category\n",
    "def find_common_words(df, category):\n",
    "        \n",
    "    \"\"\"\n",
    "    Function: find the most frequent words in the category and return the them\n",
    "    Args:\n",
    "      df(dataframe): the dataframe of articles\n",
    "      category(str): the category name\n",
    "    Return:\n",
    "      the most frequant words in the category\n",
    "    \"\"\"\n",
    "        \n",
    "    # Create dataframes for the category\n",
    "    cat_df = df[df[\"Category\"]==category]\n",
    "    \n",
    "    # Initialize words list for the category\n",
    "    words = [word for tokens in cat_df[\"Preprocessed_Text\"] for word in tokens]\n",
    "    \n",
    "    # Count words frequency\n",
    "    words_counter = Counter(words)\n",
    " \n",
    "    return words_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most common words in each category\")\n",
    "for c in category:\n",
    "    print(c, \" News\")\n",
    "    print(find_common_words(df, c))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d4916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735421c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b110a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef69aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30cc85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e67768a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['package', 'amazon', 'never', 'arrived', 'fix', 'asap']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokenization\n",
    "\n",
    "def token(txt):\n",
    "    tokenized_by_word = word_tokenize(txt)\n",
    "    return tokenized_by_word\n",
    "\n",
    "token(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "##Load language Specific .pickle file\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
    "##Different type of tokenizer\n",
    "from nltk.tokenize import regexp_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.tokenize import WordPunctTokenizer \n",
    "from nltk.tokenize import PunktWordTokenizer \n",
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "##Sample initialization of token \n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "##Define Normalization \n",
    "normalization = None\n",
    "normalization = 'stemmer'\n",
    "normalization = 'lemmatizer'\n",
    "##Define Vectorizer \n",
    "vectorizer = 'countvectorizer'\n",
    "vectorizer = 'tfidfvectorizer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ec17a",
   "metadata": {},
   "source": [
    "### 4. Stemming and Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fed079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "tokenized_string = word_tokenize(text)\n",
    "\n",
    "stemmed = [stemmer.stem(word) for word in tokenized_string]\n",
    "#Example of Lemmatization\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_string = word_tokenize(text)\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c66ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "word_list = ['rains', 'raining', 'rain', 'rained']\n",
    "ps = PorterStemmer()\n",
    "for w in word_list:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1165940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "data_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86222634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_stemmer(text):\n",
    "    stem_text = [PorterStemmer().stem(i) for i in text]\n",
    "    return stem_text\n",
    "data_clean['text_tokens_stem'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8bb563",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n",
    "    return lem_text\n",
    "data_clean['text_tokens_lemma'] = data_clean['text_tokens'].apply(lambda x: word_lemmatizer(x))\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bdf79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "def normalize_tokens(normalization):\n",
    "    if normalization is not None:\n",
    "        if normalization == 'stemmer':\n",
    "            train['text'] = train['text'].apply(stem_tokens)\n",
    "        elif normalization == 'lemmatizer':\n",
    "            train['text'] = train['text'].apply(lemmatize_tokens)\n",
    "        \n",
    "normalize_tokens(normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b814a",
   "metadata": {},
   "source": [
    "# Vectorize your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43668a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Default CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd509e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizationin funcation\n",
    "def vectorize(vectorizer):\n",
    "    if vectorizer == 'countvectorizer':\n",
    "        print('countvectorizer')\n",
    "        vectorizer = CountVectorizer()\n",
    "        train_vectors = vectorizer.fit_transform(train['text'])\n",
    "        test_vectors = vectorizer.transform(test['text'])\n",
    "    elif vectorizer == 'tfidfvectorizer':\n",
    "        print('tfidfvectorizer')\n",
    "        vectorizer = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "        train_vectors = vectorizer.fit_transform(train['text'])\n",
    "        test_vectors = vectorizer.transform(test['text'])\n",
    "    return train_vectors, test_vectors\n",
    "train_vectors, test_vectors = vectorize(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec138f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
