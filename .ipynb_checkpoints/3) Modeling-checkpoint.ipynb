{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2d11a44",
   "metadata": {},
   "source": [
    "# Articles Recommendation Categorization\n",
    "\n",
    "Recommending web articles for the learners for different study programs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79b41d",
   "metadata": {},
   "source": [
    "### 1) Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe9096b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339ed37",
   "metadata": {},
   "source": [
    "### 2) Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f88602b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load TF-IDF Vectors\n",
    "\n",
    "with open('Vectors/train_vector.pkl', 'rb') as handle:\n",
    "    X_train = pickle.load(handle)\n",
    "\n",
    "with open('Vectors/val_vector.pkl', 'rb') as handle:\n",
    "    X_val = pickle.load(handle)\n",
    "\n",
    "with open('Vectors/test_vector.pkl', 'rb') as handle:\n",
    "    X_test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a16451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load Label Vectors\n",
    "\n",
    "with open('Vectors/train_label.pkl', 'rb') as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "\n",
    "with open('Vectors/val_label.pkl', 'rb') as handle:\n",
    "    y_val = pickle.load(handle)\n",
    "\n",
    "with open('Vectors/test_label.pkl', 'rb') as handle:\n",
    "    y_test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0f02d",
   "metadata": {},
   "source": [
    "## 3) Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0582a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(gamma='auto', kernel='linear')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  train SVM model\n",
    "clf = svm.SVC(kernel='linear', gamma='auto')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#  save model\n",
    "#filename = PATH_ROOT + 'models/interpress_news_category_tr_lite_classifier_svm_model_4000.sav'\n",
    "#pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54ad9af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43,  0,  6],\n",
       "       [ 0, 63, 14],\n",
       "       [ 3, 10, 83]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting classifier to the Training set\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "\n",
    "classifier.fit(X_train , y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f31e1069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.91        49\n",
      "           1       0.86      0.82      0.84        77\n",
      "           2       0.81      0.86      0.83        96\n",
      "\n",
      "    accuracy                           0.85       222\n",
      "   macro avg       0.87      0.85      0.86       222\n",
      "weighted avg       0.85      0.85      0.85       222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "cp = classification_report(y_test, y_pred)\n",
    "print(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b544d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8513513513513513, 189)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this function computes subset accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred), accuracy_score(y_test, y_pred,normalize=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd4cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f986e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "15309d1e",
   "metadata": {},
   "source": [
    "Here, I will use the below machine learning algorithms then I will select the best one based on its training time, accuracy score and f-beta scores.\n",
    "\n",
    "Support Vector Machine\n",
    "Ada Boost\n",
    "Gradient Boosting\n",
    "XG Boost\n",
    "Decision Tree\n",
    "Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86229563",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AdaBoostClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19844/4064063789.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Initialize the models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0msv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0mgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mxgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AdaBoostClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Train and evaluate model\n",
    "def fit_eval_model(model, train_features, y_train, test_features, y_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function: train and evaluate a machine learning classifier.\n",
    "    Args:\n",
    "      model: machine learning classifier\n",
    "      train_features: train data extracted features\n",
    "      y_train: train data lables\n",
    "      test_features: train data extracted features\n",
    "      y_test: train data lables\n",
    "    Return:\n",
    "      results(dictionary): a dictionary of the model training time and classification report\n",
    "    \"\"\"\n",
    "    results ={}\n",
    "    \n",
    "    # Start time\n",
    "    start = time.time()\n",
    "    # Train the model\n",
    "    model.fit(train_features, y_train)\n",
    "    # End time\n",
    "    end = time.time()\n",
    "    # Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "    \n",
    "    # Test the model\n",
    "    train_predicted = model.predict(train_features)\n",
    "    test_predicted = model.predict(test_features)\n",
    "    \n",
    "     # Classification report\n",
    "    results['classification_report'] = classification_report(y_test, test_predicted)\n",
    "        \n",
    "    return results\n",
    "    \n",
    "# Initialize the models\n",
    "sv = svm.SVC()\n",
    "ab = AdaBoostClassifier(random_state = 1)\n",
    "gb = GradientBoostingClassifier(random_state = 1)\n",
    "xgb = xgboost.XGBClassifier(random_state = 1)\n",
    "tree = DecisionTreeClassifier()\n",
    "nb = MultinomialNB()\n",
    "\n",
    "\n",
    "# Fit and evaluate models\n",
    "results = {}\n",
    "for cls in [sv, ab, gb, xgb, tree, nb]:\n",
    "    cls_name = cls.__class__.__name__\n",
    "    results[cls_name] = {}\n",
    "    results[cls_name] = fit_eval_model(cls, train_features, y_train, test_features, y_test)\n",
    "    \n",
    "# Print classifiers results\n",
    "for res in results:\n",
    "    print (res)\n",
    "    print()\n",
    "    for i in results[res]:\n",
    "        print (i, ':')\n",
    "        print(results[res][i])\n",
    "        print()\n",
    "    print ('-----')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55081f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "engines = [('PassiveAggressiveClassifier', PassiveAggressiveClassifier(fit_intercept=True, n_jobs=-1, random_state=0)),\n",
    "           ('NearestCentroid', NearestCentroid()), \n",
    "           ('RandomForestClassifier', RandomForestClassifier(min_samples_leaf=0.01))]\n",
    "\n",
    "\n",
    "\n",
    "for name, engine in engines:\n",
    "    clf = make_pipeline(tfidf, engine).fit(xtrain, ytrain)\n",
    "    prediction = clf.predict(xtest)\n",
    "    score = clf.score(xtest, prediction)\n",
    "    with lzma.open('./data/{}.pickle.xz'.format(name.lower()), 'wb') as f:\n",
    "        pickle.dump(clf, f, protocol=5)\n",
    "\n",
    "\n",
    "s = '''\n",
    "\n",
    "‘Guys, you’ve got to hear this,” I said. I was sitting in front of my computer one day in July 2012, with one eye on a screen of share prices and the other on a live stream of the House of Commons Treasury select committee hearings. As the Barclays share price took a graceful swan dive, I pulled my headphones out of the socket and turned up the volume so everyone could hear. My colleagues left their terminals and came around to watch BBC Parliament with me.\n",
    "\n",
    "It didn’t take long to realise what was happening. “Bob’s getting murdered,” someone said.\n",
    "\n",
    "Bob Diamond, the swashbuckling chief executive of Barclays, had been called before the committee to explain exactly what his bank had been playing at in regards to the Libor rate-fixing scandal. The day before his appearance, he had made things very much worse by seeming to accuse the deputy governor of the Bank of England of ordering him to fiddle an important benchmark, then walking back the accusation as soon as it was challenged. He was trying to turn on his legendary charm in front of a committee of angry MPs, and it wasn’t working. On our trading floor, in Mayfair, calls were coming in from all over the City. Investors needed to know what was happening and whether the damage was reparable.\n",
    "\n",
    "A couple of weeks later, the damage was done. The money was gone, Diamond was out of a job and the market, as it always does, had moved on. We were left asking ourselves: How did we get it so wrong?\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "result = []\n",
    "for file in glob.glob('./data/*.pickle.xz'):\n",
    "  clf   = pickle.load(lzma.open('{}'.format(file), 'rb'))\n",
    "  ypred = clf.predict([s])\n",
    "  score = clf.score([s], ypred)\n",
    "  print(file, ypred[0], score)\n",
    "  result.append(ypred[0])\n",
    "\n",
    "print(pd.io.json.dumps(Counter(result), indent=4))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
